{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce2b1b9-6b52-4aad-8f71-321f1f6db206",
   "metadata": {},
   "source": [
    "# Interview questions about transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd060e4-c240-48b0-a00a-d89e6dac9253",
   "metadata": {},
   "source": [
    "1. Why does Transformer use multi-head attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb299a8-5db3-4ccc-b990-1c1aafe2121e",
   "metadata": {},
   "source": [
    "+ ability: Enhanced Representation Learning:\n",
    "Multi-head attention allows the model to learn different representations of the input data by applying attention multiple times with different sets of parameters. Each head attends to different parts of the input, capturing various aspects and patterns. This leads to richer and more comprehensive representations.\n",
    "+ speed: Parallelization:\n",
    "Multi-head attention allows for parallel processing, as the attention computations for each head can be done independently and in parallel. This improves the efficiency and speed of the model, making it feasible to train large-scale models on extensive datasets.\n",
    "\n",
    "The authors of the paper proved this point through experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc92fb-df44-4725-8f8c-84e29da798d8",
   "metadata": {},
   "source": [
    "2. why self-attention expression should scale for QK？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf5f64-5da8-4cc3-a6bd-46b0924f03a0",
   "metadata": {},
   "source": [
    "\n",
    "Scaling the dot product by $\\sqrt{d_k}$ in the self-attention mechanism is essential to maintain numerical stability, prevent the saturation of the softmax function, and ensure effective gradient flow during training. This small adjustment helps the Transformer models train more efficiently and perform better.\n",
    "\n",
    "scaling需要进行softmax操作，对于softmax的公式，除以维度的开根号值可以规避一部分数值会进入敏感区间，防止梯度消失，让模型能够更容易训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158e129-c965-46e4-a387-ac0f7e86b048",
   "metadata": {},
   "source": [
    "3. why transformer use LayerNorm and dont's use BatchNorm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0911b96-c812-459b-93a7-673e173049ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zklearn",
   "language": "python",
   "name": "zklearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
